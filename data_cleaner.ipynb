{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86e0e18-4f0b-4cbe-95c3-dfbb32ac4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import timeit\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb68a9-9659-4e90-b970-f9fd67250581",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Import the huge dataset as JSON and convert it to .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaacfd3-b2d9-4d82-a8a2-72066dc644c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('yelp_academic_dataset_review.json', lines=True)\n",
    "relevant_cols = ['review_id','text','stars']\n",
    "df_relevant = df[relevant_cols]\n",
    "table = pa.Table.from_pandas(df_relevant)\n",
    "pq.write_table(table, 'data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c7310a-41de-4bca-ae4c-618089a6ff9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Convert each of the star values to [0,4] rather than [1,5] (will come in handy later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f70b1e3-0758-4b5d-9105-22f30140fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4136c315-3312-46f7-958f-5e26efe6aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrease_star(star_number):\n",
    "    star_number = int(star_number)\n",
    "    return star_number -1\n",
    "df['stars'] = df.stars.apply(decrease_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d013b-1fb1-450c-9ad4-fa908ef4f86f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define our tokenizer and our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91749e16-c75c-4243-a2ed-1dac9fe8ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9c83c-5907-4fc1-af10-06345ef8b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d92ef1-986a-4f9a-a3c8-d457b4da6ced",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tokenize and encode the data in batches of 1000 rows each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5e2f7-dbfc-4688-afa5-6111b64a9c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_batches = int(len(df)/batch_size)\n",
    "print(f\"Number of batches to run: {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e2c4b-b650-41eb-93cc-2a836bb7cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_list = [] # Add list to store the encoded value of the tokens\n",
    "attention_mask_list = []  # Add list to store attention masks\n",
    "token_type_ids_list = []  # Add list to store segment IDs\n",
    "\n",
    "final_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462fd8ba-0f86-4938-98a2-a15ceaaaff19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(num_batches):\n",
    "    \n",
    "    # Prints out a nice messsage\n",
    "    print(f\"Processing batch {i}/{num_batches}\")\n",
    "    \n",
    "    # Only looks at the section of the huge df that we're interested in for this batch\n",
    "    batch_df = df[i*batch_size:(i+1)*batch_size]\n",
    "    \n",
    "    # Encode the batch of data\n",
    "    for txt in batch_df.text:\n",
    "        tokens = tokenizer.encode_plus(\n",
    "            txt,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # Append the input IDs, attention mask, and segment IDs to their respective lists\n",
    "        input_ids_list.append(tokens['input_ids'][0].tolist())\n",
    "        attention_mask_list.append(tokens['attention_mask'][0].tolist())\n",
    "        token_type_ids_list.append(tokens['token_type_ids'][0].tolist())\n",
    "        \n",
    "    # The first write \n",
    "    if i == 100:\n",
    "        # Make a new df that contains the list of encoded data for this set of batches\n",
    "        final_df = pd.DataFrame()\n",
    "        final_df['input_ids'] = pd.Series(input_ids_list)\n",
    "\n",
    "        # Convert the DataFrame to a PyArrow table\n",
    "        table = pa.Table.from_pandas(final_df)\n",
    "\n",
    "        # Write the PyArrow table to a new Parquet file\n",
    "        pq.write_table(table, 'final_data.parquet')\n",
    "\n",
    "        # Reset the lists\n",
    "        input_ids_list = [] \n",
    "        attention_mask_list = [] \n",
    "        token_type_ids_list = []  \n",
    "\n",
    "    # All the rest of the writes\n",
    "    if i % 100 == 0 and i > 100 :\n",
    "\n",
    "        # Make a new df that contains the list of encoded data for this set of batches\n",
    "        final_df = pd.DataFrame()\n",
    "        final_df['input_ids'] = pd.Series(input_ids_list)\n",
    "\n",
    "        # Load the existing Parquet file into a PyArrow table\n",
    "        existing_table = pq.read_table('final_data.parquet')\n",
    "\n",
    "        # Convert the DataFrame to a PyArrow table\n",
    "        new_table = pa.Table.from_pandas(final_df)\n",
    "\n",
    "        # Concatenate the existing table and the new table\n",
    "        concatenated_table = pa.concat_tables([existing_table, new_table])\n",
    "\n",
    "        # Write the concatenated table to the same Parquet file\n",
    "        pq.write_table(concatenated_table, 'final_data.parquet')\n",
    "\n",
    "        # Reset the lists\n",
    "        input_ids_list = [] \n",
    "        attention_mask_list = [] \n",
    "        token_type_ids_list = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
