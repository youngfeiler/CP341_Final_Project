{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06aa32f9-caa9-455d-8a5f-466a4873a855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ab38039-cdeb-4640-a7d0-432d26180d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"small_tokenized_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12b05a05-f00e-434c-bbf5-2bf4d960cf3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars                                               text  \\\n",
      "0      2  If you decide to eat here, just be aware it is...   \n",
      "1      4  I've taken a lot of spin classes over the year...   \n",
      "2      2  Family diner. Had the buffet. Eclectic assortm...   \n",
      "3      4  Wow!  Yummy, different,  delicious.   Our favo...   \n",
      "4      3  Cute interior and owner (?) gave us tour of up...   \n",
      "\n",
      "                                           input_ids  \\\n",
      "0  [101, 2065, 2017, 5630, 2000, 4521, 2182, 1010...   \n",
      "1  [101, 1045, 1005, 2310, 2579, 1037, 2843, 1997...   \n",
      "2  [101, 2155, 15736, 1012, 2018, 1996, 28305, 10...   \n",
      "3  [101, 10166, 999, 9805, 18879, 1010, 2367, 101...   \n",
      "4  [101, 10140, 4592, 1998, 3954, 1006, 1029, 100...   \n",
      "\n",
      "                                      attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                      token_type_ids  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "IDS = df[\"input_ids\"].tolist()\n",
    "newIDS=[]\n",
    "for each in IDS:\n",
    "    #end_index = next((i for i, x in enumerate(each) if x==0), len(each))\n",
    "    # Remove the trailing 0's\n",
    "    #each = each[:end_index]\n",
    "    # Convert the input to a tensor\n",
    "    input_tensor = torch.tensor(each).unsqueeze(0)\n",
    "    newIDS.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ec3437a-78ec-4744-a2c3-aecd556a5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the attention mask\n",
    "attn_mask_list = df[\"attention_mask\"].tolist()\n",
    "attn_mask=[]\n",
    "for each in attn_mask_list:\n",
    "    # Convert the input to a tensor\n",
    "    input_tensor = torch.tensor(each).unsqueeze(0)\n",
    "    attn_mask.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "630b610d-a4c7-47ed-887e-383701777f43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get the star count AKA the y from half_data\n",
    "stars = df[\"stars\"].tolist()\n",
    "starIDS=[]\n",
    "for each in stars:\n",
    "    #end_index = next((i for i, x in enumerate(each) if x==0), len(each))\n",
    "    # Remove the trailing 0's\n",
    "    #each = each[:end_index]\n",
    "    # Convert the input to a tensor\n",
    "    input_tensor = torch.tensor(each).unsqueeze(0)\n",
    "    starIDS.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97095140-0351-476b-a715-2e9948f612ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bca9a566-a853-4a52-917c-dce73c09fb06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load data in batches for bert\n",
    "bertData = TensorDataset(torch.tensor(newIDS), torch.tensor(attn_mask), torch.tensor(starIDS))\n",
    "# set up a data loader to get batches from our dataset\n",
    "bertLoader = torch.utils.data.DataLoader(bertData, batch_size = 16, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8513b-5c40-48c2-8159-582eb7db4a75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 768])\n",
      "tensor([[-0.7095, -0.7251, -0.9989,  ..., -0.9867, -0.7343,  0.4843],\n",
      "        [-0.4864, -0.6547, -0.9994,  ..., -0.9906, -0.5134,  0.0553],\n",
      "        [-0.5880, -0.6714, -0.9982,  ..., -0.9911, -0.6021,  0.2036],\n",
      "        ...,\n",
      "        [-0.3518, -0.4987, -0.9953,  ..., -0.9548, -0.3662, -0.0789],\n",
      "        [-0.6950, -0.7400, -0.9994,  ..., -0.9914, -0.6979,  0.4265],\n",
      "        [-0.6581, -0.7117, -0.9995,  ..., -0.9957, -0.7217,  0.4861]],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "0\n",
      "torch.Size([16, 768])\n",
      "tensor([[-0.4350, -0.6582, -0.9986,  ..., -0.9881, -0.4199, -0.1506],\n",
      "        [-0.4150, -0.4869, -0.9962,  ..., -0.9677, -0.4544, -0.1006],\n",
      "        [-0.4036, -0.7195, -0.9978,  ..., -0.9821, -0.6394,  0.2457],\n",
      "        ...,\n",
      "        [-0.4309, -0.5282, -0.9944,  ..., -0.9701, -0.4067,  0.3970],\n",
      "        [-0.6601, -0.6068, -0.9959,  ..., -0.9822, -0.5707,  0.4432],\n",
      "        [-0.5120, -0.6687, -0.9953,  ..., -0.9637, -0.5847,  0.1854]],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "1\n",
      "torch.Size([16, 768])\n",
      "tensor([[-0.7048, -0.4846, -0.9831,  ..., -0.9778, -0.6431,  0.8170],\n",
      "        [-0.6373, -0.6819, -0.9976,  ..., -0.9866, -0.6673,  0.3901],\n",
      "        [-0.5416, -0.6706, -0.9975,  ..., -0.9628, -0.6230,  0.5057],\n",
      "        ...,\n",
      "        [-0.6484, -0.6443, -0.9967,  ..., -0.9678, -0.6438,  0.2570],\n",
      "        [-0.5549, -0.6618, -0.9950,  ..., -0.9615, -0.5471,  0.1438],\n",
      "        [-0.1551, -0.4507, -0.9933,  ..., -0.9742, -0.1874, -0.0690]],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for batch in bertLoader:\n",
    "    y, pooled = bert(batch[0])\n",
    "    print(pooled.shape)\n",
    "    print(pooled)\n",
    "\n",
    "    torch.save(pooled, (\"vector\"+\"%d\"+\".txt\")%i)\n",
    "        \n",
    "    torch.save(batch[1], (\"label\"+\"%d\"+\".txt\")%i)\n",
    "\n",
    "    print(i)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef5bad-3775-4b9d-a203-97a40db6c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...final_pool = np.concatinate(list_pool/files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864917fa-e2de-4150-adf1-d81b4a7915a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
